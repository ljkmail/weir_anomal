import streamlit as st
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup, Comment
from datetime import datetime, timedelta, date
from scipy import stats
from statsmodels.tsa.seasonal import STL
import plotly.express as px
import plotly.graph_objects as go
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from math import floor
import re
from requests.adapters import HTTPAdapter, Retry
import warnings

st.set_page_config(page_title="ì˜ì‚°ê°•ë³´ ì§€í•˜ìˆ˜ì¸¡ì •ë§ ì´ìƒì¹˜ ê²€ìƒ‰", layout="wide")

st.title("ì˜ì‚°ê°•ë³´ ì§€í•˜ìˆ˜ì¸¡ì •ë§ ì´ìƒì¹˜ ê²€ìƒ‰ ì„œë¹„ìŠ¤")

# --------------------------------------------------------------
# Sidebar: ì‚¬ìš©ì ì…ë ¥
# --------------------------------------------------------------
st.sidebar.header("ì„¤ì •")
max_anoms_frac = st.sidebar.slider("ìµœëŒ€ ì´ìƒì¹˜ ë¹„ìœ¨ (max_anoms, fraction)", 0.005, 0.1, 0.01, step=0.005)
alpha = st.sidebar.slider("ìœ ì˜ìˆ˜ì¤€ Î± (GESDì—ì„œ ì‚¬ìš©í•˜ëŠ” í†µê³„ì  ìœ ì˜ìˆ˜ì¤€)", 0.001, 0.10, 0.05, step=0.001)
anomal_day = st.sidebar.slider("ìµœê·¼ ëª‡ ì¼ ì•ˆì˜ ì´ìƒì„ ê²€ìƒ‰í• ì§€(ì¼)", 1, 30, 7, step=1)
max_workers = st.sidebar.slider("ë™ì‹œ ìš”ì²­(ìŠ¤ë ˆë“œ) ìˆ˜", 4, 32, 8, step=1)
use_decomposition = st.sidebar.checkbox("ì‹œê³„ì—´ ë¶„í•´ ì‚¬ìš© (anomalize ë°©ì‹)", value=True)
run_button = st.sidebar.button("ì´ìƒê°’ ê²€ì¶œ ì‹œì‘ ğŸ”")


# --------------------------------------------------------------
# GESD implementation
# --------------------------------------------------------------
def generalized_esd(arr, max_anoms_count, alpha):
    """Generalized ESD Test for Outliers"""
    x = arr.copy().astype(float)
    n = len(x)
    if n == 0 or max_anoms_count < 1:
        return []

    R = []
    lam = []
    removed_idx = []
    x_work = x.copy()
    idx_map = list(range(n))

    for r in range(1, max_anoms_count + 1):
        mu = np.mean(x_work)
        sigma = np.std(x_work, ddof=1)
        if sigma == 0 or np.isnan(sigma):
            break
        abs_dev = np.abs(x_work - mu)
        max_idx_local = int(np.nanargmax(abs_dev))
        Ri = abs_dev[max_idx_local] / sigma
        R.append(Ri)

        p = 1 - alpha / (2 * (n - r + 1))
        df = n - r - 1
        if df <= 0:
            lam.append(np.inf)
        else:
            t_dist = stats.t.ppf(p, df)
            numerator = (n - r) * t_dist
            denominator = np.sqrt((df + t_dist**2) * (n - r + 1))
            lambda_r = numerator / denominator
            lam.append(lambda_r)

        removed_idx.append(idx_map.pop(max_idx_local))
        x_work = np.delete(x_work, max_idx_local)
        if len(x_work) < 3:
            break

    k = 0
    for i in range(len(R)):
        if R[i] > lam[i]:
            k = i + 1
    return removed_idx[:k]


# --------------------------------------------------------------
# Time Series Decomposition (anomalize ë°©ì‹) - ìˆ˜ì •ëœ ë²„ì „
# --------------------------------------------------------------
def time_decompose(df, value_col, freq=7):
    """
    ì‹œê³„ì—´ ë¶„í•´: ê´€ì¸¡ê°’ = ì¶”ì„¸(trend) + ê³„ì ˆì„±(seasonal) + ì”ì°¨(remainder)
    Rì˜ anomalize::time_decompose()ì™€ ìœ ì‚¬í•œ ë°©ì‹
    - STL í˜¸ì¶œ ì‹œ ë‚˜ì˜¤ëŠ” ê²½ê³ /ë©”ì‹œì§€ë¥¼ UIì— ë…¸ì¶œí•˜ì§€ ì•Šê³  ì¡°ìš©íˆ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    try:
        # ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
        if len(df) < freq * 2:
            # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ ê°„ë‹¨í•œ ì´ë™í‰ê· ìœ¼ë¡œ ì¶”ì„¸ ì¶”ì¶œ
            window = min(7, max(3, len(df)//2))
            df['trend'] = df[value_col].rolling(window=window, center=True, min_periods=1).mean()
            df['seasonal'] = 0
            df['remainder'] = df[value_col] - df['trend']
            return df

        # STL ë¶„í•´: statsmodelsì˜ STLì€ period/seasonal ì¸ìë¥¼ ë°›ìŒ. 
        # STL/fit ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ê²½ê³ ë¥¼ UIë¡œ ë…¸ì¶œí•˜ì§€ ì•Šë„ë¡ warningsë¥¼ ì»¨íŠ¸ë¡¤í•©ë‹ˆë‹¤.
        with warnings.catch_warnings():
            # STLì´ë‚˜ ë‚´ë¶€ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” PeriodWarning ë“± ê²½ê³ ë¥¼ ì–µì œ
            warnings.filterwarnings("ignore")
            stl = STL(df[value_col].fillna(method='ffill').fillna(method='bfill').to_numpy(),
                      seasonal=freq, trend=None)
            result = stl.fit()

        # result.trend/seasonal/residì€ numpy ë°°ì—´ì´ë¯€ë¡œ DataFrameì— ë‹¤ì‹œ ë„£ê¸°
        df = df.reset_index(drop=True)
        df['trend'] = result.trend
        df['seasonal'] = result.seasonal
        df['remainder'] = result.resid

        return df

    except Exception:
        # ì˜ˆì™¸ ë°œìƒ ì‹œ ì¡°ìš©íˆ ë‹¨ìˆœ ë¶„í•´ ë°©ì‹ìœ¼ë¡œ ëŒ€ì²´ (ì‚¬ìš©ìì—ê²Œ ê²½ê³  ë©”ì‹œì§€ ë…¸ì¶œ ì•ˆ í•¨)
        window = min(7, max(3, len(df)//2))
        df['trend'] = df[value_col].rolling(window=window, center=True, min_periods=1).mean()
        df['seasonal'] = 0
        df['remainder'] = df[value_col] - df['trend']
        return df

def time_recompose(df, anomaly_indices):
    """
    ì‹œê³„ì—´ ì¬êµ¬ì„± ë° ì´ìƒì¹˜ ê²½ê³„ ê³„ì‚°
    Rì˜ anomalize::time_recompose()ì™€ ìœ ì‚¬
    """
    # DataFrameì„ reset_indexí•˜ì—¬ ì •ìˆ˜ ì¸ë±ìŠ¤ ì‚¬ìš©
    df = df.reset_index(drop=True)
    
    df['anomaly'] = 'No'
    # ì •ìˆ˜ ìœ„ì¹˜(iloc)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ í‘œì‹œ
    if len(anomaly_indices) > 0:
        df.loc[anomaly_indices, 'anomaly'] = 'Yes'
    
    # ì •ìƒ ë²”ìœ„ ê³„ì‚° (ì¶”ì„¸ + ê³„ì ˆì„± Â± 3*ì”ì°¨ì˜ í‘œì¤€í¸ì°¨)
    remainder_std = df['remainder'].std()
    df['recomposed_l1'] = df['trend'] + df['seasonal'] - 3 * remainder_std
    df['recomposed_l2'] = df['trend'] + df['seasonal'] - 2 * remainder_std
    df['recomposed_l3'] = df['trend'] + df['seasonal'] - 1 * remainder_std
    df['observed'] = df['trend'] + df['seasonal'] + df['remainder']
    df['recomposed_u1'] = df['trend'] + df['seasonal'] + 1 * remainder_std
    df['recomposed_u2'] = df['trend'] + df['seasonal'] + 2 * remainder_std
    df['recomposed_u3'] = df['trend'] + df['seasonal'] + 3 * remainder_std
    
    return df


# --------------------------------------------------------------
# ë°ì´í„° ë¡œë“œ ë° ìœ í‹¸ í•¨ìˆ˜
# --------------------------------------------------------------
@st.cache_data(show_spinner=False)
def load_local_df2(path="./input/df2.csv"):
    df = pd.read_csv(path)
    if "V1" in df.columns:
        df = df.drop(columns=["V1"])
    df.columns = df.columns.str.strip()
    return df


def detect_date_col(df):
    date_candidates = [c for c in df.columns if ("date" in c.lower()) or ("time" in c.lower()) or ("valuedatetime" in c.lower())]
    if date_candidates:
        return date_candidates[0]
    for c in df.columns:
        try:
            parsed = pd.to_datetime(df[c], errors="coerce")
            if parsed.notna().sum() > 0.5 * len(parsed):
                return c
        except Exception:
            pass
    return None


def detect_level_col(df):
    candidates = ["gw_level_daily", "gw_level", "datavalue", "value", "level"]
    for cand in candidates:
        for col in df.columns:
            if col.lower() == cand:
                return col
    for col in df.columns:
        lname = col.lower()
        if "gw" in lname and "level" in lname:
            return col
        if "water" in lname or "ìˆ˜ìœ„" in lname or "level" in lname:
            return col
    return None


def detect_gennum_col(df):
    candidates = ["gennum", "resultid", "station", "gennm", "site", "code"]
    for cand in candidates:
        for col in df.columns:
            if col.lower() == cand:
                return col
    for col in df.columns:
        lname = col.lower()
        if "gen" in lname or "station" in lname or "site" in lname or "code" in lname:
            return col
    return None


# --------------------------------------------------------------
# ê°œì„ ëœ í¬ë¡¤ë§ í•¨ìˆ˜
# --------------------------------------------------------------
def create_retry_session(retries=3, backoff_factor=0.5):
    session = requests.Session()
    retry = Retry(total=retries, backoff_factor=backoff_factor, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retry)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session


def crawl_station_list(url_weir):
    pattern = re.compile(r"(?:SCM|SCMR|SCMA|SCC|JSM|JSMR|JSMA)-\d{3}")
    sess = create_retry_session()
    r = sess.get(url_weir, timeout=20)
    soup = BeautifulSoup(r.text, "html.parser")
    comments = soup.find_all(string=lambda text: isinstance(text, Comment))
    ids = set()
    for c in comments:
        ids.update(pattern.findall(c))
    if not ids:
        text = soup.get_text(" ", strip=True)
        ids.update(pattern.findall(text))
    return sorted(ids)


def fetch_station_json(gennum, from_date, to_date, session=None):
    url = "http://www.gims.go.kr/odmUndergroundChartJson"
    params = {"resultId": gennum, "fromDate": from_date, "toDate": to_date}
    sess = session or create_retry_session()
    try:
        resp = sess.get(url, params=params, timeout=20)
        resp.raise_for_status()
        js = resp.json()
        if not isinstance(js, dict) or "list" not in js or not js["list"]:
            return None
        df_temp = pd.DataFrame(js["list"])
        if "valuedatetimech" in df_temp.columns:
            df_temp["valuedatetimech"] = pd.to_datetime(df_temp["valuedatetimech"], errors="coerce")
        elif "valuedatetime" in df_temp.columns:
            df_temp["valuedatetimech"] = pd.to_datetime(df_temp["valuedatetime"], errors="coerce")
        df_temp["gennum"] = gennum
        return df_temp
    except Exception:
        return None


# --------------------------------------------------------------
# ë©”ì¸ ì‹¤í–‰
# --------------------------------------------------------------
if run_button:
    try:
        df2 = load_local_df2()
        date_col = detect_date_col(df2)
        level_col = detect_level_col(df2)
        gennum_col = detect_gennum_col(df2)

        df2 = df2.rename(columns={date_col: "valuedatetimech", gennum_col: "gennum", level_col: "gw_level_raw"})
        df2["valuedatetimech"] = pd.to_datetime(df2["valuedatetimech"], errors="coerce")
        df2 = df2.dropna(subset=["valuedatetimech"])
        df2["valuedatetimech"] = df2["valuedatetimech"].dt.date
        df2["gw_level_raw"] = pd.to_numeric(df2["gw_level_raw"], errors="coerce")
        df2_daily = df2.groupby(["gennum", "valuedatetimech"], as_index=False)["gw_level_raw"].mean().rename(columns={"gw_level_raw": "gw_level_daily"})
        df2 = df2_daily.copy()
        df2 = df2[df2["valuedatetimech"] != date(2024, 7, 31)]

        st.info("ê´€ì¸¡ì†Œ ëª©ë¡ì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤...")
        url_weir = "http://www.gims.go.kr/odmUnderground?resultId=JSM-008&fromDate=2023-04-01&toDate=2023-04-03"
        try:
            station_list = crawl_station_list(url_weir)
        except Exception as e:
            st.error(f"ê´€ì¸¡ì†Œ ëª©ë¡ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            st.stop()

        st.write(f"í¬ë¡¤ëœ ê´€ì¸¡ì†Œ ìˆ˜: {len(station_list)}")

        last_local_date = pd.to_datetime(df2["valuedatetimech"]).dt.date.max()
        from_date = last_local_date.isoformat()
        to_date = date.today().isoformat()

        st.info(f"ìƒˆ ê´€ì¸¡ìë£Œ ìˆ˜ì§‘ ì¤‘... ({from_date} ~ {to_date})")
        progress_bar = st.progress(0)
        fetched_frames = []
        failures = []

        with requests.Session() as session:
            total = len(station_list)
            with ThreadPoolExecutor(max_workers=max_workers) as exe:
                futures = {exe.submit(fetch_station_json, s, from_date, to_date, session): s for s in station_list}
                done = 0
                for fut in as_completed(futures):
                    s = futures[fut]
                    done += 1
                    progress_bar.progress(done / total)
                    try:
                        df_temp = fut.result()
                        if df_temp is not None and not df_temp.empty:
                            fetched_frames.append(df_temp)
                    except Exception as e:
                        failures.append((s, str(e)))

        st.write(f"ìˆ˜ì§‘ ì™„ë£Œ: ì„±ê³µ {len(fetched_frames)} / ì‹¤íŒ¨ {len(failures)}")

        if len(fetched_frames) == 0:
            combined = df2.copy()
        else:
            df_new = pd.concat(fetched_frames, ignore_index=True)
            df_new_pivot = df_new.pivot_table(index=["gennum", "valuedatetimech"], columns="datatype", values="datavalue", aggfunc="mean").reset_index()
            rename_map = {"01": "gw_level", "02": "river_up", "03": "river_down", "04": "rain", "05": "temper"}
            df_new_pivot = df_new_pivot.rename(columns=rename_map)
            df_new_pivot["valuedatetimech"] = pd.to_datetime(df_new_pivot["valuedatetimech"], errors="coerce").dt.date
            if "gw_level" in df_new_pivot.columns:
                df_url3 = df_new_pivot.groupby(["gennum", "valuedatetimech"], as_index=False)["gw_level"].mean().rename(columns={"gw_level": "gw_level_daily"})
            else:
                df_url3 = pd.DataFrame(columns=["gennum", "valuedatetimech", "gw_level_daily"])

            df_url3 = df_url3[df_url3["valuedatetimech"] > last_local_date]
            df_url3["gw_level_daily"] = pd.to_numeric(df_url3["gw_level_daily"], errors="coerce")
            combined = pd.concat([df2, df_url3], ignore_index=True)

        # -------------------------------
        # ì´ìƒì¹˜ íƒì§€ (anomalize ë°©ì‹)
        # -------------------------------
        st.info("ì´ìƒì¹˜ë¥¼ ê²€ìƒ‰ì¤‘ì…ë‹ˆë‹¤...")
        df2_anomal = {}
        results = []
        unique_sites = combined["gennum"].unique()
        pbar = st.progress(0)
        tot = len(unique_sites)

        # ìµœê·¼ Nì¼ ê¸°ì¤€ ë‚ ì§œ ê³„ì‚°
        recent_cut = date.today() - timedelta(days=anomal_day)

        for idx, site in enumerate(unique_sites):
            pbar.progress((idx + 1) / tot)
            
            # ê´€ì¸¡ì†Œë³„ ë°ì´í„° ì¶”ì¶œ ë° ì •ë ¬ (ì¸ë±ìŠ¤ ë¦¬ì…‹)
            df2_temp = combined[combined["gennum"] == site].dropna(subset=["gw_level_daily"]).sort_values("valuedatetimech").reset_index(drop=True).copy()
            
            if df2_temp.shape[0] < 6:
                continue
            
            # ì‹œê³„ì—´ ë¶„í•´ ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
            if use_decomposition:
                # 1. time_decompose: ì¶”ì„¸, ê³„ì ˆì„±, ì”ì°¨ ë¶„í•´
                df2_temp = time_decompose(df2_temp, 'gw_level_daily', freq=7)
                
                # 2. anomalize: ì”ì°¨(remainder)ì— ëŒ€í•´ GESD ì ìš©
                remainder_arr = df2_temp['remainder'].fillna(0).to_numpy()
            else:
                # ë¶„í•´ ì—†ì´ ì›ë³¸ ë°ì´í„°ì— ì§ì ‘ GESD ì ìš©
                remainder_arr = df2_temp['gw_level_daily'].to_numpy()
            
            n = len(remainder_arr)
            max_anoms_count = max(1, floor(max_anoms_frac * n))
            
            # GESD ì´ìƒì¹˜ íƒì§€
            anomalous_idx_list = generalized_esd(remainder_arr, max_anoms_count, alpha)
            
            if use_decomposition:
                # 3. time_recompose: ê²°ê³¼ ì¬êµ¬ì„± ë° ê²½ê³„ ê³„ì‚°
                df2_temp = time_recompose(df2_temp, anomalous_idx_list)
            else:
                # ë‹¨ìˆœ ë°©ì‹: í‰ê·  Â± 3Ïƒ ì‚¬ìš©
                df2_temp['anomaly'] = 'No'
                if len(anomalous_idx_list) > 0:
                    df2_temp.loc[anomalous_idx_list, 'anomaly'] = 'Yes'
                
                mean_val = np.mean(remainder_arr)
                std_val = np.std(remainder_arr, ddof=1)
                df2_temp['observed'] = df2_temp['gw_level_daily']
                df2_temp['recomposed_l3'] = mean_val - 3 * std_val
                df2_temp['recomposed_l2'] = mean_val - 2 * std_val
                df2_temp['recomposed_l1'] = mean_val - 1 * std_val
                df2_temp['recomposed_u1'] = mean_val + 1 * std_val
                df2_temp['recomposed_u2'] = mean_val + 2 * std_val
                df2_temp['recomposed_u3'] = mean_val + 3 * std_val
            
            # ìµœê·¼ Nì¼ ë‚´ ì´ìƒì¹˜ í™•ì¸
            recent_data = df2_temp[df2_temp['valuedatetimech'] >= recent_cut]
            has_recent_anomaly = (recent_data['anomaly'] == 'Yes').any()
            
            # ìµœê·¼ Nì¼ ë‚´ ì´ìƒì¹˜ê°€ ìˆìœ¼ë©´ ì €ì¥
            if has_recent_anomaly:
                df2_anomal[site] = df2_temp
                
                # í†µê³„ ì •ë³´ ê³„ì‚°
                last_val = df2_temp['gw_level_daily'].iloc[-1]
                mean_val = df2_temp['gw_level_daily'].mean()
                std_val = df2_temp['gw_level_daily'].std()
                anomaly_count = (df2_temp['anomaly'] == 'Yes').sum()
                recent_anomaly_count = (recent_data['anomaly'] == 'Yes').sum()
                anomaly_dates = df2_temp[df2_temp['anomaly'] == 'Yes']['valuedatetimech'].tolist()
                
                results.append({
                    "ê´€ì¸¡ì†Œëª…": site,
                    "ì´ìƒìƒí™©": "ìˆ˜ìœ„ìë£Œí™•ì¸í•„ìš”",
                    "í•´ë°œìˆ˜ìœ„": last_val,
                    "í‰ê· ìˆ˜ìœ„": mean_val,
                    "í‘œì¤€í¸ì°¨": std_val,
                    "ì´ìƒì¹˜ê°œìˆ˜": anomaly_count,
                    "ìµœê·¼ì´ìƒì¹˜ê°œìˆ˜": recent_anomaly_count,
                    "anomaly_dates": ", ".join([str(d) for d in anomaly_dates]),
                    "recent_anomaly_flag": True
                })

        st.success("ì´ìƒì¹˜ íƒì§€ ì™„ë£Œ!")

        # -------------------------------
        # ë¯¸ìˆ˜ì‹  ê´€ì¸¡ì†Œ í™•ì¸
        # -------------------------------
        st.info("ë¯¸ìˆ˜ì‹  ê´€ì¸¡ì†Œ í™•ì¸ ì¤‘...")
        recent_data = combined[combined["valuedatetimech"] >= recent_cut]

        missing_sites = []
        for site in station_list:
            site_data = recent_data[recent_data["gennum"] == site]
            valid_days = site_data.dropna(subset=["gw_level_daily"])["valuedatetimech"].nunique()
            
            # ìµœê·¼ Nì¼ ì¤‘ í•˜ë£¨ë¼ë„ ëˆ„ë½ëœ ê²½ìš° ë¯¸ìˆ˜ì‹ ìœ¼ë¡œ ë¶„ë¥˜
            if valid_days < anomal_day:
                missing_sites.append(site)

        for ms in missing_sites:
            # ì¤‘ë³µ ë°©ì§€: ì´ë¯¸ resultsì— ìˆëŠ” ê´€ì¸¡ì†ŒëŠ” ì œì™¸
            if not any(r["ê´€ì¸¡ì†Œëª…"] == ms for r in results):
                results.append({
                    "ê´€ì¸¡ì†Œëª…": ms,
                    "ì´ìƒìƒí™©": "ë¯¸ìˆ˜ì‹ ",
                    "í•´ë°œìˆ˜ìœ„": np.nan,
                    "í‰ê· ìˆ˜ìœ„": np.nan,
                    "í‘œì¤€í¸ì°¨": np.nan,
                    "ì´ìƒì¹˜ê°œìˆ˜": 0,
                    "ìµœê·¼ì´ìƒì¹˜ê°œìˆ˜": 0,
                    "anomaly_dates": "",
                    "recent_anomaly_flag": False
                })

        # -------------------------------
        # ê²°ê³¼ ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° í•„í„°ë§
        # -------------------------------
        df_results = pd.DataFrame(results).drop_duplicates(subset=["ê´€ì¸¡ì†Œëª…"])

        # ìµœê·¼ Nì¼ ë‚´ ì´ìƒì¹˜ OR ë¯¸ìˆ˜ì‹  ê´€ì¸¡ì†Œë§Œ í•„í„°ë§
        df_results_filtered = df_results[
            (df_results["ì´ìƒìƒí™©"] == "ë¯¸ìˆ˜ì‹ ") | (df_results["recent_anomaly_flag"])
        ].sort_values("ê´€ì¸¡ì†Œëª…")

        # -------------------------------
        # ê²°ê³¼ ìš”ì•½ í‘œì‹œ
        # -------------------------------
        st.subheader(f"ğŸ” ìµœê·¼ {anomal_day}ì¼ ë‚´ ì´ìƒì¹˜ / ë¯¸ìˆ˜ì‹  ê´€ì¸¡ì†Œ ìš”ì•½")

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì´ ê´€ì¸¡ì†Œ ìˆ˜", len(station_list))
        with col2:
            anomaly_count = df_results_filtered[df_results_filtered["ì´ìƒìƒí™©"] == "ìˆ˜ìœ„ìë£Œí™•ì¸í•„ìš”"].shape[0]
            st.metric("ì´ìƒì¹˜ ê´€ì¸¡ì†Œ", anomaly_count)
        with col3:
            missing_count = df_results_filtered[df_results_filtered["ì´ìƒìƒí™©"] == "ë¯¸ìˆ˜ì‹ "].shape[0]
            st.metric("ë¯¸ìˆ˜ì‹  ê´€ì¸¡ì†Œ", missing_count)

        if df_results_filtered.empty:
            st.success(f"âœ… ìµœê·¼ {anomal_day}ì¼ ë‚´ ì´ìƒì¹˜ ë˜ëŠ” ë¯¸ìˆ˜ì‹  ê´€ì¸¡ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # ê²°ê³¼ í…Œì´ë¸” í‘œì‹œ
            display_cols = ["ê´€ì¸¡ì†Œëª…", "ì´ìƒìƒí™©", "í•´ë°œìˆ˜ìœ„", "í‰ê· ìˆ˜ìœ„", "í‘œì¤€í¸ì°¨", "ì´ìƒì¹˜ê°œìˆ˜", "ìµœê·¼ì´ìƒì¹˜ê°œìˆ˜", "anomaly_dates"]
            st.dataframe(
                df_results_filtered[display_cols].reset_index(drop=True).style.format({
                    "í•´ë°œìˆ˜ìœ„": "{:.2f}",
                    "í‰ê· ìˆ˜ìœ„": "{:.2f}",
                    "í‘œì¤€í¸ì°¨": "{:.2f}"
                }, na_rep="-"),
                use_container_width=True
            )
            
            # -------------------------------
            # ì´ìƒì¹˜ ì‹œê°í™” (anomalize ìŠ¤íƒ€ì¼)
            # -------------------------------
            st.subheader(f"ğŸ“Š ìµœê·¼ {anomal_day}ì¼ ë‚´ ì´ìƒì¹˜ ê´€ì¸¡ì†Œ ì‹œê°í™”")
            
            # ìµœê·¼ ì´ìƒì¹˜ê°€ ìˆëŠ” ê´€ì¸¡ì†Œë§Œ í•„í„°ë§
            sites_with_recent_anomalies = df_results_filtered[
                (df_results_filtered["recent_anomaly_flag"]) & 
                (df_results_filtered["ì´ìƒìƒí™©"] == "ìˆ˜ìœ„ìë£Œí™•ì¸í•„ìš”")
            ]["ê´€ì¸¡ì†Œëª…"].tolist()
            
            if not sites_with_recent_anomalies:
                st.info("ì‹œê°í™”í•  ìµœê·¼ ì´ìƒì¹˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                # ê´€ì¸¡ì†Œ ì„ íƒ ì˜µì…˜ (ì„¸ì…˜ ìƒíƒœë¡œ ì„ íƒ ìœ ì§€)
                # sites_with_recent_anomalies ê°€ ë¹„ì—ˆìœ¼ë©´ ìœ„ì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë¹„ì–´ìˆì§€ ì•ŠìŒ ê°€ì •
                if 'selected_station' not in st.session_state or st.session_state['selected_station'] not in sites_with_recent_anomalies:
                    # ì„¸ì…˜ ìƒíƒœì— ê°’ì´ ì—†ê±°ë‚˜ í˜„ì¬ ëª©ë¡ì— ì—†ëŠ” ê°’ì´ë©´ ì²« í•­ëª©ìœ¼ë¡œ ì´ˆê¸°í™”
                    st.session_state['selected_station'] = sites_with_recent_anomalies[0]

                selected_station = st.selectbox(
                    "ê´€ì¸¡ì†Œ ì„ íƒ",
                    options=sites_with_recent_anomalies,
                    key='selected_station'
                )
                
                if selected_station in df2_anomal:
                    plot_data = df2_anomal[selected_station].copy()
                    
                    # Plotly ê·¸ë˜í”„ ìƒì„± (anomalize ìŠ¤íƒ€ì¼)
                    fig = go.Figure()
                    
                    # ì •ìƒ ë²”ìœ„ ìŒì˜ (3Ïƒ)
                    fig.add_trace(go.Scatter(
                        x=plot_data['valuedatetimech'],
                        y=plot_data['recomposed_u3'],
                        mode='lines',
                        line=dict(color='rgba(200, 200, 200, 0)'),
                        showlegend=False,
                        hoverinfo='skip'
                    ))
                    
                    fig.add_trace(go.Scatter(
                        x=plot_data['valuedatetimech'],
                        y=plot_data['recomposed_l3'],
                        mode='lines',
                        line=dict(color='rgba(200, 200, 200, 0)'),
                        fill='tonexty',
                        fillcolor='rgba(200, 200, 200, 0.1)',
                        name='ì •ìƒ ë²”ìœ„ (Â±3Ïƒ)',
                        hoverinfo='skip'
                    ))
                    
                    # 2Ïƒ ë²”ìœ„
                    fig.add_trace(go.Scatter(
                        x=plot_data['valuedatetimech'],
                        y=plot_data['recomposed_u2'],
                        mode='lines',
                        line=dict(color='rgba(150, 150, 150, 0)'),
                        showlegend=False,
                        hoverinfo='skip'
                    ))
                    
                    fig.add_trace(go.Scatter(
                        x=plot_data['valuedatetimech'],
                        y=plot_data['recomposed_l2'],
                        mode='lines',
                        line=dict(color='rgba(150, 150, 150, 0)'),
                        fill='tonexty',
                        fillcolor='rgba(150, 150, 150, 0.1)',
                        name='ê²½ê³  ë²”ìœ„ (Â±2Ïƒ)',
                        hoverinfo='skip'
                    ))
                    
                    # ê´€ì¸¡ê°’ (ì •ìƒ)
                    normal_data = plot_data[plot_data['anomaly'] == 'No']
                    fig.add_trace(go.Scatter(
                        x=normal_data['valuedatetimech'],
                        y=normal_data['observed'],
                        mode='lines+markers',
                        line=dict(color='steelblue', width=2),
                        marker=dict(size=5),
                        name='ê´€ì¸¡ê°’ (ì •ìƒ)'
                    ))
                    
                    # ì´ìƒì¹˜ (ê³¼ê±°)
                    anomaly_data = plot_data[(plot_data['anomaly'] == 'Yes') & 
                                            (plot_data['valuedatetimech'] < recent_cut)]
                    if not anomaly_data.empty:
                        fig.add_trace(go.Scatter(
                            x=anomaly_data['valuedatetimech'],
                            y=anomaly_data['observed'],
                            mode='markers',
                            marker=dict(color='orange', size=12, symbol='x', line=dict(width=2)),
                            name='ê³¼ê±° ì´ìƒì¹˜'
                        ))
                    
                    # ì´ìƒì¹˜ (ìµœê·¼)
                    recent_anomaly_data = plot_data[(plot_data['anomaly'] == 'Yes') & 
                                                   (plot_data['valuedatetimech'] >= recent_cut)]
                    if not recent_anomaly_data.empty:
                        fig.add_trace(go.Scatter(
                            x=recent_anomaly_data['valuedatetimech'],
                            y=recent_anomaly_data['observed'],
                            mode='markers',
                            marker=dict(color='red', size=15, symbol='diamond', line=dict(width=2, color='darkred')),
                            name=f'ìµœê·¼ {anomal_day}ì¼ ì´ìƒì¹˜'
                        ))
                    
                    # ì¶”ì„¸ì„  (ì‹œê³„ì—´ ë¶„í•´ ì‚¬ìš© ì‹œ)
                    if use_decomposition and 'trend' in plot_data.columns:
                        fig.add_trace(go.Scatter(
                            x=plot_data['valuedatetimech'],
                            y=plot_data['trend'] + plot_data['seasonal'],
                            mode='lines',
                            line=dict(color='green', dash='dash', width=2),
                            name='ì¶”ì„¸ + ê³„ì ˆì„±'
                        ))
                    
                    # ìµœê·¼ Nì¼ êµ¬ê°„ ê°•ì¡°
                    fig.add_vrect(
                        x0=recent_cut,
                        x1=date.today(),
                        fillcolor="rgba(255, 0, 0, 0.05)",
                        layer="below",
                        line_width=0,
                        annotation_text=f"ìµœê·¼ {anomal_day}ì¼",
                        annotation_position="top left"
                    )
                    
                    fig.update_layout(
                        title=f"ğŸŒŠ {selected_station} ì§€í•˜ìˆ˜ìœ„ ì‹œê³„ì—´ (anomalize ë°©ì‹ ì´ìƒì¹˜ íƒì§€)",
                        xaxis_title="ë‚ ì§œ",
                        yaxis_title="ì§€í•˜ìˆ˜ìœ„ (m)",
                        height=600,
                        hovermode="x unified",
                        legend=dict(
                            orientation="h",
                            yanchor="bottom",
                            y=1.02,
                            xanchor="right",
                            x=1
                        )
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # ìƒì„¸ í†µê³„ ì •ë³´
                    info_row = df_results_filtered[df_results_filtered["ê´€ì¸¡ì†Œëª…"] == selected_station].iloc[0]
                    
                    st.subheader("ğŸ“Š í†µê³„ ì •ë³´")
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("í‰ê·  ìˆ˜ìœ„", f"{info_row['í‰ê· ìˆ˜ìœ„']:.2f} m")
                    with col2:
                        st.metric("í‘œì¤€í¸ì°¨", f"{info_row['í‘œì¤€í¸ì°¨']:.2f} m")
                    with col3:
                        st.metric("ì „ì²´ ì´ìƒì¹˜", f"{info_row['ì´ìƒì¹˜ê°œìˆ˜']}ê°œ")
                    with col4:
                        st.metric("ìµœê·¼ ì´ìƒì¹˜", f"{info_row['ìµœê·¼ì´ìƒì¹˜ê°œìˆ˜']}ê°œ")
                    
                    st.write(f"**ì´ìƒì¹˜ ë°œìƒ ë‚ ì§œ:** {info_row['anomaly_dates']}")
                    
                    # ì‹œê³„ì—´ ë¶„í•´ ê²°ê³¼ (ì‚¬ìš© ì‹œ)
                    if use_decomposition and all(col in plot_data.columns for col in ['trend', 'seasonal', 'remainder']):
                        with st.expander("ğŸ“ˆ ì‹œê³„ì—´ ë¶„í•´ ê²°ê³¼ (Time Decomposition)"):
                            fig_decomp = go.Figure()
                            
                            # ì„œë¸Œí”Œë¡¯ ìƒì„±
                            from plotly.subplots import make_subplots
                            
                            fig_decomp = make_subplots(
                                rows=4, cols=1,
                                subplot_titles=('ì›ë³¸ ê´€ì¸¡ê°’ (Observed)', 'ì¶”ì„¸ (Trend)', 
                                              'ê³„ì ˆì„± (Seasonal)', 'ì”ì°¨ (Remainder)'),
                                vertical_spacing=0.08
                            )
                            
                            # ì›ë³¸
                            fig_decomp.add_trace(
                                go.Scatter(x=plot_data['valuedatetimech'], y=plot_data['observed'],
                                          mode='lines', name='Observed', line=dict(color='steelblue')),
                                row=1, col=1
                            )
                            
                            # ì¶”ì„¸
                            fig_decomp.add_trace(
                                go.Scatter(x=plot_data['valuedatetimech'], y=plot_data['trend'],
                                          mode='lines', name='Trend', line=dict(color='green')),
                                row=2, col=1
                            )
                            
                            # ê³„ì ˆì„±
                            fig_decomp.add_trace(
                                go.Scatter(x=plot_data['valuedatetimech'], y=plot_data['seasonal'],
                                          mode='lines', name='Seasonal', line=dict(color='orange')),
                                row=3, col=1
                            )
                            
                            # ì”ì°¨ (ì´ìƒì¹˜ í‘œì‹œ)
                            remainder_normal = plot_data[plot_data['anomaly'] == 'No']
                            remainder_anomaly = plot_data[plot_data['anomaly'] == 'Yes']
                            
                            fig_decomp.add_trace(
                                go.Scatter(x=remainder_normal['valuedatetimech'], y=remainder_normal['remainder'],
                                          mode='lines', name='Remainder', line=dict(color='purple')),
                                row=4, col=1
                            )
                            
                            if not remainder_anomaly.empty:
                                fig_decomp.add_trace(
                                    go.Scatter(x=remainder_anomaly['valuedatetimech'], y=remainder_anomaly['remainder'],
                                              mode='markers', name='Anomaly', 
                                              marker=dict(color='red', size=10, symbol='diamond')),
                                    row=4, col=1
                                )
                            
                            fig_decomp.update_xaxes(title_text="ë‚ ì§œ", row=4, col=1)
                            fig_decomp.update_yaxes(title_text="ìˆ˜ìœ„ (m)", row=1, col=1)
                            fig_decomp.update_yaxes(title_text="ìˆ˜ìœ„ (m)", row=2, col=1)
                            fig_decomp.update_yaxes(title_text="ìˆ˜ìœ„ (m)", row=3, col=1)
                            fig_decomp.update_yaxes(title_text="ìˆ˜ìœ„ (m)", row=4, col=1)
                            
                            fig_decomp.update_layout(
                                height=800,
                                showlegend=False,
                                title_text=f"{selected_station} ì‹œê³„ì—´ ë¶„í•´ ë¶„ì„"
                            )
                            
                            st.plotly_chart(fig_decomp, use_container_width=True)
                    
                    # ë°ì´í„° í…Œì´ë¸”
                    with st.expander("ğŸ“„ ì „ì²´ ë°ì´í„° ë³´ê¸°"):
                        display_columns = ['valuedatetimech', 'gw_level_daily', 'anomaly']
                        if use_decomposition:
                            display_columns.extend(['trend', 'seasonal', 'remainder', 'observed'])
                        
                        st.dataframe(
                            plot_data[display_columns].style.format({
                                'gw_level_daily': '{:.3f}',
                                'trend': '{:.3f}',
                                'seasonal': '{:.3f}',
                                'remainder': '{:.3f}',
                                'observed': '{:.3f}'
                            }, na_rep='-'),
                            use_container_width=True
                        )
                    
                    # ì´ìƒì¹˜ ìƒì„¸ ì •ë³´
                    with st.expander("âš ï¸ ì´ìƒì¹˜ ìƒì„¸ ì •ë³´"):
                        anomaly_details = plot_data[plot_data['anomaly'] == 'Yes'][
                            ['valuedatetimech', 'gw_level_daily', 'observed']
                        ].copy()
                        
                        if not anomaly_details.empty:
                            anomaly_details['ìµœê·¼ì—¬ë¶€'] = anomaly_details['valuedatetimech'].apply(
                                lambda x: 'ìµœê·¼' if x >= recent_cut else 'ê³¼ê±°'
                            )
                            anomaly_details = anomaly_details.rename(columns={
                                'valuedatetimech': 'ë‚ ì§œ',
                                'gw_level_daily': 'ì§€í•˜ìˆ˜ìœ„',
                                'observed': 'ê´€ì¸¡ê°’'
                            })
                            
                            st.dataframe(
                                anomaly_details.style.format({
                                    'ì§€í•˜ìˆ˜ìœ„': '{:.3f}',
                                    'ê´€ì¸¡ê°’': '{:.3f}'
                                }),
                                use_container_width=True
                            )
                        else:
                            st.info("ì´ìƒì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        st.code(traceback.format_exc())

else:
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì •ì„ ì¡°ì •í•œ í›„ 'ì´ìƒê°’ ê²€ì¶œ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
    st.markdown("""
    ### ì‚¬ìš© ë°©ë²•
    1. **ìµœëŒ€ ì´ìƒì¹˜ ë¹„ìœ¨**: ì „ì²´ ë°ì´í„° ì¤‘ ì´ìƒì¹˜ë¡œ ê°„ì£¼í•  ìµœëŒ€ ë¹„ìœ¨ ì„¤ì •
    2. **ìœ ì˜ìˆ˜ì¤€ Î±**: í†µê³„ì  ê²€ì •ì˜ ìœ ì˜ìˆ˜ì¤€ (ë‚®ì„ìˆ˜ë¡ ì—„ê²©)
    3. **ê²€ìƒ‰ ì¼ìˆ˜**: ìµœê·¼ ë©°ì¹  ë™ì•ˆì˜ ì´ìƒì¹˜ë¥¼ í™•ì¸í• ì§€ ì„¤ì •
    4. **ë™ì‹œ ìš”ì²­ ìˆ˜**: ë°ì´í„° ìˆ˜ì§‘ ì‹œ ë™ì‹œì— ì²˜ë¦¬í•  ìŠ¤ë ˆë“œ ìˆ˜
    5. **ì‹œê³„ì—´ ë¶„í•´ ì‚¬ìš©**: Rì˜ anomalize íŒ¨í‚¤ì§€ì™€ ë™ì¼í•œ ë°©ì‹ ì ìš© ì—¬ë¶€
    
    ### ê¸°ëŠ¥
    - âœ… **GESD** (Generalized Extreme Studentized Deviate) ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
    - âœ… **ì‹œê³„ì—´ ë¶„í•´** (Time Decomposition): ì¶”ì„¸, ê³„ì ˆì„±, ì”ì°¨ ë¶„ë¦¬
    - âœ… **anomalize ë°©ì‹**: Rì˜ anomalize íŒ¨í‚¤ì§€ì™€ ë™ì¼í•œ ì›Œí¬í”Œë¡œìš°
      1. `time_decompose()`: STL ë¶„í•´ë¡œ ì¶”ì„¸/ê³„ì ˆì„±/ì”ì°¨ ì¶”ì¶œ
      2. `anomalize()`: ì”ì°¨ì— GESD ì ìš©í•˜ì—¬ ì´ìƒì¹˜ íƒì§€
      3. `time_recompose()`: ê²°ê³¼ ì¬êµ¬ì„± ë° ì‹ ë¢°êµ¬ê°„ ê³„ì‚°
    - âœ… ì‹¤ì‹œê°„ ê´€ì¸¡ì†Œ ë°ì´í„° ìˆ˜ì§‘
    - âœ… ë¯¸ìˆ˜ì‹  ê´€ì¸¡ì†Œ ìë™ ê°ì§€
    - âœ… ì‹œê°í™”ë¥¼ í†µí•œ ì´ìƒì¹˜ í™•ì¸
    
    ### ì‹œê³„ì—´ ë¶„í•´ (Time Decomposition)
    - **ì¶”ì„¸ (Trend)**: ì¥ê¸°ì ì¸ ìƒìŠ¹/í•˜ë½ ê²½í–¥
    - **ê³„ì ˆì„± (Seasonal)**: ì£¼ê¸°ì ìœ¼ë¡œ ë°˜ë³µë˜ëŠ” íŒ¨í„´ (ê¸°ë³¸ 7ì¼ ì£¼ê¸°)
    - **ì”ì°¨ (Remainder)**: ì¶”ì„¸ì™€ ê³„ì ˆì„±ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë³€ë™
    - ì”ì°¨ì—ì„œ GESDë¥¼ ì ìš©í•˜ì—¬ ì´ìƒì¹˜ë¥¼ ë” ì •í™•í•˜ê²Œ íƒì§€í•©ë‹ˆë‹¤.
    """)